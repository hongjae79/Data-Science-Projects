{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('./train.csv')\n",
    "df_test = pd.read_csv('./test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_core = df_train[['airline_sentiment', 'text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3338\n"
     ]
    }
   ],
   "source": [
    "#Check if there is an nan.  nan is float.  \n",
    "\n",
    "for i in range(len(df_train_core)):\n",
    "    if pd.isna(df_train_core.text[i]) == True:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#So let's delete row 3338\n",
    "\n",
    "df_train_core_without_nan = df_train_core.drop([3338])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('@united', 2722),\n",
       " ('to', 1884),\n",
       " ('the', 1309),\n",
       " ('I', 1064),\n",
       " ('a', 1041),\n",
       " ('for', 842),\n",
       " ('you', 785),\n",
       " ('and', 753),\n",
       " ('on', 729),\n",
       " ('my', 702),\n",
       " ('is', 598),\n",
       " ('in', 574),\n",
       " ('flight', 554),\n",
       " ('of', 482),\n",
       " ('@VirginAmerica', 424),\n",
       " ('your', 368),\n",
       " ('have', 368),\n",
       " ('with', 349),\n",
       " ('me', 341),\n",
       " ('it', 333),\n",
       " ('was', 332),\n",
       " ('at', 324),\n",
       " ('that', 303),\n",
       " ('not', 292),\n",
       " ('from', 291),\n",
       " ('get', 258),\n",
       " ('be', 257),\n",
       " ('but', 239),\n",
       " ('no', 237),\n",
       " ('this', 220),\n",
       " ('are', 214),\n",
       " ('an', 212),\n",
       " ('can', 180),\n",
       " (\"I'm\", 175),\n",
       " ('we', 173),\n",
       " ('just', 172),\n",
       " ('Cancelled', 168),\n",
       " ('-', 164),\n",
       " ('so', 160),\n",
       " ('customer', 152),\n",
       " ('been', 147),\n",
       " ('service', 146),\n",
       " ('will', 145),\n",
       " ('time', 140),\n",
       " ('do', 140),\n",
       " ('Flight', 135),\n",
       " ('now', 134),\n",
       " ('they', 127),\n",
       " ('would', 127),\n",
       " ('about', 124),\n",
       " ('up', 123),\n",
       " ('flights', 118),\n",
       " ('when', 114),\n",
       " ('out', 114),\n",
       " ('our', 112),\n",
       " ('as', 112),\n",
       " ('one', 109),\n",
       " ('if', 109),\n",
       " ('or', 109),\n",
       " ('plane', 108),\n",
       " ('delayed', 108),\n",
       " ('has', 108),\n",
       " ('there', 104),\n",
       " ('like', 104),\n",
       " ('us', 103),\n",
       " ('bag', 100),\n",
       " ('gate', 99),\n",
       " ('still', 98),\n",
       " ('how', 98),\n",
       " ('all', 96),\n",
       " ('had', 96),\n",
       " ('2', 95),\n",
       " ('help', 95),\n",
       " ('Late', 95),\n",
       " ('need', 94),\n",
       " ('why', 93),\n",
       " ('what', 91),\n",
       " ('United', 91),\n",
       " ('@United', 89),\n",
       " ('hours', 87),\n",
       " ('after', 86),\n",
       " ('am', 85),\n",
       " (\"don't\", 85),\n",
       " (\"it's\", 84),\n",
       " (\"can't\", 84),\n",
       " ('more', 80),\n",
       " ('back', 80),\n",
       " ('by', 79),\n",
       " ('thanks', 79),\n",
       " ('&amp', 78),\n",
       " ('airline', 78),\n",
       " ('got', 78),\n",
       " ('hour', 76),\n",
       " ('because', 75),\n",
       " ('waiting', 72),\n",
       " ('last', 71),\n",
       " ('3', 71),\n",
       " ('fly', 69),\n",
       " ('Thanks', 69),\n",
       " ('thank', 69),\n",
       " ('going', 68),\n",
       " ('over', 67),\n",
       " ('trying', 67),\n",
       " ('You', 66),\n",
       " ('lost', 66),\n",
       " ('make', 66),\n",
       " ('flight.', 65),\n",
       " ('flying', 64),\n",
       " ('know', 64),\n",
       " ('should', 64),\n",
       " ('Thank', 64),\n",
       " ('first', 63),\n",
       " ('any', 63),\n",
       " ('were', 63),\n",
       " ('way', 62),\n",
       " ('then', 62),\n",
       " ('seat', 61),\n",
       " ('check', 61),\n",
       " ('i', 60),\n",
       " ('Flightled', 60),\n",
       " ('only', 59),\n",
       " ('people', 59),\n",
       " ('due', 57),\n",
       " ('wait', 57),\n",
       " ('see', 57),\n",
       " ('did', 56),\n",
       " ('Just', 55),\n",
       " ('take', 54),\n",
       " ('go', 54),\n",
       " ('good', 53),\n",
       " ('This', 53),\n",
       " ('guys', 53),\n",
       " ('please', 53),\n",
       " ('sent', 53),\n",
       " ('luggage', 52),\n",
       " ('DM', 52),\n",
       " ('baggage', 52),\n",
       " ('4', 52),\n",
       " ('u', 51),\n",
       " ('airport', 51),\n",
       " ('really', 50),\n",
       " ('&amp;', 50),\n",
       " ('than', 50),\n",
       " ('getting', 50),\n",
       " ('ticket', 50),\n",
       " ('even', 50),\n",
       " ('two', 50),\n",
       " ('miss', 49),\n",
       " ('Can', 49),\n",
       " ('staff', 49),\n",
       " ('never', 49),\n",
       " ('day', 49),\n",
       " ('delay', 48),\n",
       " ('told', 48),\n",
       " ('let', 48),\n",
       " ('again', 48),\n",
       " ('bags', 48),\n",
       " ('another', 47),\n",
       " ('you.', 47),\n",
       " ('next', 47),\n",
       " ('Your', 47),\n",
       " (\"I've\", 46),\n",
       " ('want', 46),\n",
       " ('crew', 46),\n",
       " ('made', 46),\n",
       " ('could', 45),\n",
       " ('minutes', 45),\n",
       " ('through', 45),\n",
       " ('agent', 45),\n",
       " ('off', 44),\n",
       " ('We', 44),\n",
       " ('home', 44),\n",
       " ('email', 44),\n",
       " ('What', 43),\n",
       " ('me.', 43),\n",
       " ('very', 43),\n",
       " ('booked', 43),\n",
       " ('new', 43),\n",
       " ('who', 43),\n",
       " ('into', 43),\n",
       " ('before', 43),\n",
       " (\"I'll\", 43),\n",
       " ('their', 43),\n",
       " ('sure', 43),\n",
       " (\"didn't\", 42),\n",
       " ('seats', 42),\n",
       " ('change', 42),\n",
       " ('great', 41),\n",
       " ('trip', 41),\n",
       " ('other', 41),\n",
       " ('Booking', 41),\n",
       " ('No', 41),\n",
       " ('it.', 41),\n",
       " ('give', 41),\n",
       " ('boarding', 41),\n",
       " ('phone', 40),\n",
       " ('call', 40),\n",
       " ('How', 40),\n",
       " ('being', 40),\n",
       " ('sitting', 40),\n",
       " ('checked', 40),\n",
       " ('UA', 40),\n",
       " ('min', 39),\n",
       " ('1', 39),\n",
       " ('care', 39),\n",
       " ('worst', 39),\n",
       " ('SFO', 38),\n",
       " ('number', 38),\n",
       " ('Is', 38),\n",
       " ('someone', 38),\n",
       " ('website', 38),\n",
       " ('same', 38),\n",
       " ('you!', 38),\n",
       " ('ever', 38),\n",
       " ('The', 38),\n",
       " ('experience', 37),\n",
       " ('book', 37),\n",
       " ('hold', 37),\n",
       " ('Not', 37),\n",
       " ('left', 36),\n",
       " ('think', 36),\n",
       " ('does', 36),\n",
       " ('It', 36),\n",
       " ('long', 36),\n",
       " ('Problems', 35),\n",
       " ('its', 35),\n",
       " ('much', 35),\n",
       " ('EWR', 34),\n",
       " ('class', 34),\n",
       " ('Please', 34),\n",
       " ('Why', 34),\n",
       " ('them', 33),\n",
       " ('person', 33),\n",
       " (\"that's\", 33),\n",
       " ('connection', 33),\n",
       " ('use', 32),\n",
       " (\"It's\", 32),\n",
       " ('days', 32),\n",
       " ('her', 32),\n",
       " (\"doesn't\", 32),\n",
       " ('he', 32),\n",
       " ('ORD', 32),\n",
       " ('hrs', 31),\n",
       " ('here', 31),\n",
       " ('better', 31),\n",
       " ('some', 31),\n",
       " ('My', 31),\n",
       " ('times', 30),\n",
       " ('find', 30),\n",
       " ('missed', 30),\n",
       " ('travel', 29),\n",
       " ('passengers', 29),\n",
       " ('customers', 29),\n",
       " ('said', 29),\n",
       " ('work', 29),\n",
       " ('board', 29),\n",
       " ('service.', 29),\n",
       " ('since', 29),\n",
       " ('she', 29),\n",
       " ('1st', 29),\n",
       " ('yes', 29),\n",
       " ('united', 29),\n",
       " (':)', 28),\n",
       " ('status', 28),\n",
       " (\"won't\", 28),\n",
       " ('via', 28),\n",
       " ('Will', 28),\n",
       " ('Do', 28),\n",
       " (\"you're\", 28),\n",
       " ('which', 28),\n",
       " ('stuck', 28),\n",
       " ('claim', 28),\n",
       " ('Flighted', 28),\n",
       " ('already', 28),\n",
       " ('Now', 28),\n",
       " ('agents', 28),\n",
       " ('without', 28),\n",
       " ('20', 28),\n",
       " ('pay', 27),\n",
       " ('tried', 27),\n",
       " ('If', 27),\n",
       " ('put', 27),\n",
       " ('too', 26),\n",
       " ('So', 26),\n",
       " ('LAX', 26),\n",
       " ('upgrade', 26),\n",
       " ('business', 26),\n",
       " ('connecting', 26),\n",
       " ('tell', 26),\n",
       " ('again.', 26),\n",
       " ('every', 25),\n",
       " ('love', 25),\n",
       " ('able', 25),\n",
       " ('But', 25),\n",
       " ('until', 25),\n",
       " ('right', 25),\n",
       " ('And', 25),\n",
       " ('hotel', 25),\n",
       " ('weather', 25),\n",
       " ('contact', 25),\n",
       " ('now.', 25),\n",
       " ('follow', 25),\n",
       " ('problem', 25),\n",
       " ('missing', 25),\n",
       " ('look', 25),\n",
       " ('A', 25),\n",
       " ('Denver', 25),\n",
       " ('making', 24),\n",
       " ('free', 24),\n",
       " ('working', 24),\n",
       " ('called', 24),\n",
       " ('line', 24),\n",
       " ('refund', 24),\n",
       " ('today', 24),\n",
       " ('5', 24),\n",
       " ('also', 24),\n",
       " ('app', 24),\n",
       " ('Houston', 24),\n",
       " ('They', 24),\n",
       " (\"That's\", 24),\n",
       " ('tomorrow', 24),\n",
       " ('where', 24),\n",
       " ('#UnitedAirlines', 24),\n",
       " ('6', 23),\n",
       " ('today.', 23),\n",
       " ('start', 23),\n",
       " ('earlier', 23),\n",
       " ('team', 23),\n",
       " ('issue', 23),\n",
       " ('says', 23),\n",
       " ('hope', 23),\n",
       " ('response', 23),\n",
       " ('those', 23),\n",
       " ('miles', 23),\n",
       " ('bad', 22),\n",
       " ('Thanks!', 22),\n",
       " ('least', 22),\n",
       " ('airlines', 22),\n",
       " ('many', 22),\n",
       " ('10', 22),\n",
       " ('attendant', 22),\n",
       " ('airline.', 22),\n",
       " ('finally', 22),\n",
       " ('plus', 21),\n",
       " ('say', 21),\n",
       " ('online', 21),\n",
       " ('IAD', 21),\n",
       " ('Any', 21),\n",
       " ('different', 21),\n",
       " ('down', 21),\n",
       " ('San', 21),\n",
       " ('time.', 21),\n",
       " ('extra', 21),\n",
       " ('understand', 21),\n",
       " ('departure', 21),\n",
       " ('help.', 21),\n",
       " ('paid', 21),\n",
       " ('took', 21),\n",
       " ('system', 21),\n",
       " ('charge', 21),\n",
       " ('IAH', 21),\n",
       " ('Flightr', 21),\n",
       " ('site', 20),\n",
       " ('add', 20),\n",
       " ('return', 20),\n",
       " ('30', 20),\n",
       " (\"isn't\", 20),\n",
       " ('received', 20),\n",
       " ('done', 20),\n",
       " ('well', 20),\n",
       " ('ground', 20),\n",
       " ('his', 20),\n",
       " ('terrible', 20),\n",
       " ('@united,', 20),\n",
       " ('name', 19),\n",
       " ('having', 19),\n",
       " ('Thanks.', 19),\n",
       " ('reservation', 19),\n",
       " (\"what's\", 19),\n",
       " ('8', 19),\n",
       " ('mechanical', 19),\n",
       " ('delays', 19),\n",
       " ('broken', 19),\n",
       " ('while', 19),\n",
       " ('@AmericanAir', 19),\n",
       " ('wifi', 19),\n",
       " ('car', 19),\n",
       " ('hear', 19),\n",
       " ('issues', 19),\n",
       " ('Chicago', 19),\n",
       " ('‚Äú@united:', 19),\n",
       " ('thanks.', 19),\n",
       " ('.@united', 19),\n",
       " ('week', 18),\n",
       " ('Need', 18),\n",
       " ('ago', 18),\n",
       " ('needs', 18),\n",
       " ('flight,', 18),\n",
       " ('.', 18),\n",
       " ('here.', 18),\n",
       " ('come', 18),\n",
       " ('best', 18),\n",
       " ('tonight.', 18),\n",
       " ('flt', 18),\n",
       " ('fix', 18),\n",
       " (\"I'd\", 18),\n",
       " ('something', 18),\n",
       " ('early', 18),\n",
       " ('half', 18),\n",
       " ('try', 18),\n",
       " ('Newark', 18),\n",
       " ('info', 18),\n",
       " ('mins', 18),\n",
       " ('me?', 17),\n",
       " ('All', 17),\n",
       " ('Was', 17),\n",
       " ('weeks', 17),\n",
       " ('landed', 17),\n",
       " ('both', 17),\n",
       " ('doing', 17),\n",
       " ('When', 17),\n",
       " ('leave', 17),\n",
       " ('old', 17),\n",
       " ('Still', 17),\n",
       " ('ask', 17),\n",
       " ('update', 17),\n",
       " ('actually', 17),\n",
       " ('send', 17),\n",
       " ('own', 17),\n",
       " ('appreciate', 17),\n",
       " ('helpful', 17),\n",
       " ('#', 17),\n",
       " ('changed', 17),\n",
       " ('available', 16),\n",
       " ('ur', 16),\n",
       " ('night', 16),\n",
       " ('awesome', 16),\n",
       " ('disappointed', 16),\n",
       " ('went', 16),\n",
       " ('direct', 16),\n",
       " ('rep', 16),\n",
       " ('might', 16),\n",
       " ('open', 16),\n",
       " ('stranded', 16),\n",
       " ('confirmation', 16),\n",
       " ('looking', 16),\n",
       " (\"wasn't\", 16),\n",
       " ('second', 16),\n",
       " ('pilot', 16),\n",
       " ('taking', 16),\n",
       " ('poor', 16),\n",
       " ('room', 16),\n",
       " ('international', 16),\n",
       " ('almost', 16),\n",
       " ('employees', 16),\n",
       " ('Customer', 16),\n",
       " ('asked', 16),\n",
       " ('nice', 16),\n",
       " ('hours.', 16),\n",
       " ('rebooked', 16),\n",
       " ('thing', 15),\n",
       " ('yes,', 15),\n",
       " ('such', 15),\n",
       " ('heard', 15),\n",
       " ('gave', 15),\n",
       " (':(', 15),\n",
       " ('this?', 15),\n",
       " ('?', 15),\n",
       " ('speak', 15),\n",
       " ('anyone', 15),\n",
       " ('On', 15),\n",
       " ('show', 15),\n",
       " ('landing', 15),\n",
       " ('lot', 15),\n",
       " ('full', 15),\n",
       " ('may', 15),\n",
       " ('less', 15),\n",
       " ('chance', 15),\n",
       " ('planes', 15),\n",
       " ('Never', 15),\n",
       " ('instead', 15),\n",
       " ('each', 15),\n",
       " ('gate.', 15),\n",
       " ('out.', 15),\n",
       " ('given', 15),\n",
       " ('@', 15),\n",
       " ('7', 15),\n",
       " ('him', 15),\n",
       " ('case', 15),\n",
       " ('yet', 15),\n",
       " ('maintenance', 15),\n",
       " (\"you've\", 14),\n",
       " ('@virginamerica', 14),\n",
       " ('nothing', 14),\n",
       " ('At', 14),\n",
       " ('year', 14),\n",
       " ('though', 14),\n",
       " ('She', 14),\n",
       " ('credit', 14),\n",
       " ('bag.', 14),\n",
       " ('morning', 14),\n",
       " ('hung', 14),\n",
       " ('seems', 14),\n",
       " ('That', 14),\n",
       " ('minute', 14),\n",
       " ('luggage.', 14),\n",
       " ('Worst', 14),\n",
       " ('w/', 14),\n",
       " ('that.', 14),\n",
       " ('day.', 14),\n",
       " ('group', 14),\n",
       " ('voucher', 14),\n",
       " ('sit', 14),\n",
       " ('always', 14),\n",
       " ('far', 14),\n",
       " ('45', 14),\n",
       " ('away', 13),\n",
       " ('arrived', 13),\n",
       " ('Virgin', 13),\n",
       " ('three', 13),\n",
       " ('...', 13),\n",
       " ('anything', 13),\n",
       " (\"Don't\", 13),\n",
       " ('amazing', 13),\n",
       " ('hard', 13),\n",
       " ('Have', 13),\n",
       " ('this.', 13),\n",
       " ('@SouthwestAir', 13),\n",
       " (\"Can't\", 13),\n",
       " ('most', 13),\n",
       " ('ever.', 13),\n",
       " ('calling', 13),\n",
       " ('policy', 13),\n",
       " ('tickets', 13),\n",
       " ('happy', 13),\n",
       " ('desk', 13),\n",
       " ('wrong', 13),\n",
       " ('job', 13),\n",
       " ('makes', 13),\n",
       " ('hoping', 13),\n",
       " ('tonight', 13),\n",
       " ('yr', 13),\n",
       " ('reason', 13),\n",
       " ('24', 13),\n",
       " ('company', 13),\n",
       " ('did.', 13),\n",
       " ('hr', 13),\n",
       " ('In', 13),\n",
       " ('tarmac', 13),\n",
       " ('U', 13),\n",
       " ('traveling', 13),\n",
       " ('form', 13),\n",
       " ('vacation', 13),\n",
       " ('Flight.', 13),\n",
       " ('message', 13),\n",
       " ('@Delta', 13),\n",
       " ('enough', 13),\n",
       " ('mean', 12),\n",
       " ('little', 12),\n",
       " ('big', 12),\n",
       " ('2nd', 12),\n",
       " ('option', 12),\n",
       " (\"haven't\", 12),\n",
       " (',', 12),\n",
       " ('once', 12),\n",
       " ('share', 12),\n",
       " ('assistance', 12),\n",
       " ('coming', 12),\n",
       " ('respond', 12),\n",
       " ('rebook', 12),\n",
       " ('tomorrow.', 12),\n",
       " ('cabin', 12),\n",
       " ('row', 12),\n",
       " ('past', 12),\n",
       " ('looks', 12),\n",
       " ('mileage', 12),\n",
       " ('15', 12),\n",
       " ('stay', 12),\n",
       " ('#united', 12),\n",
       " ('flights.', 12),\n",
       " ('talk', 12),\n",
       " ('overnight', 12),\n",
       " ('used', 12),\n",
       " ('snow', 12),\n",
       " ('rude', 12),\n",
       " ('@united.', 12),\n",
       " ('rather', 12),\n",
       " ('everyone', 12),\n",
       " ('complaint', 12),\n",
       " ('real', 12),\n",
       " ('file', 12),\n",
       " ('delay.', 12),\n",
       " ('sleep', 12),\n",
       " ('@united:', 12),\n",
       " ('reFlight', 12),\n",
       " ('reply.', 12),\n",
       " ('happened', 11),\n",
       " ('#fail', 11),\n",
       " ('air', 11),\n",
       " ('RT', 11),\n",
       " ('leaving', 11),\n",
       " ('Feb', 11),\n",
       " ('After', 11),\n",
       " ('flew', 11),\n",
       " ('again!', 11),\n",
       " ('#customerservice', 11),\n",
       " ('scheduled', 11),\n",
       " ('Love', 11),\n",
       " ('Gate', 11),\n",
       " ('40', 11),\n",
       " ('link', 11),\n",
       " ('sorry', 11),\n",
       " ('yesterday', 11),\n",
       " ('lose', 11),\n",
       " ('forward', 11),\n",
       " ('Good', 11),\n",
       " ('First', 11),\n",
       " ('DM.', 11),\n",
       " ('safety', 11),\n",
       " ('standby', 11),\n",
       " ('keep', 11),\n",
       " ('forced', 11),\n",
       " ('longer', 11),\n",
       " ('saying', 11),\n",
       " ('question', 11),\n",
       " ('maybe', 11),\n",
       " ('pass', 11),\n",
       " ('50', 11),\n",
       " ('details', 11),\n",
       " ('bc', 11),\n",
       " ('asking', 11),\n",
       " ('top', 11),\n",
       " ('terminal', 11),\n",
       " ('figure', 11),\n",
       " ('plane.', 11),\n",
       " (\"couldn't\", 11),\n",
       " ('+', 11),\n",
       " (\"we're\", 11),\n",
       " ('There', 11),\n",
       " ('emailed', 10),\n",
       " ('food', 10),\n",
       " ('ago.', 10),\n",
       " ('shows', 10),\n",
       " ('award', 10),\n",
       " ('reservation.', 10),\n",
       " ('number.', 10),\n",
       " ('morning.', 10),\n",
       " ('using', 10),\n",
       " ('video', 10),\n",
       " ('on.', 10),\n",
       " ('moved', 10),\n",
       " ('process', 10),\n",
       " ('giving', 10),\n",
       " ('checking', 10),\n",
       " ('in.', 10),\n",
       " ('checkin', 10),\n",
       " ('Had', 10),\n",
       " (':', 10),\n",
       " (\"What's\", 10),\n",
       " ('Very', 10),\n",
       " ('filed', 10),\n",
       " ('destination', 10),\n",
       " ('entire', 10),\n",
       " ('stop', 10),\n",
       " ('price', 10),\n",
       " ('several', 10),\n",
       " ('carry', 10),\n",
       " ('empty', 10),\n",
       " ('help?', 10),\n",
       " ('soon', 10),\n",
       " ('between', 10),\n",
       " ('overhead', 10),\n",
       " ('lack', 10),\n",
       " ('flight!', 10),\n",
       " ('guy', 10),\n",
       " ('telling', 10),\n",
       " ('these', 10),\n",
       " ('Pls', 10),\n",
       " ('them.', 10),\n",
       " ('worth', 10),\n",
       " ('up.', 10),\n",
       " ('arrival', 10),\n",
       " ('Thx', 10),\n",
       " ('airport.', 10),\n",
       " ('compensation', 10),\n",
       " ('NOT', 10),\n",
       " ('night.', 10),\n",
       " ('fact', 10),\n",
       " ('fee', 10),\n",
       " ('means', 10),\n",
       " ('Tarmac', 10),\n",
       " ('Flightled.', 10),\n",
       " (\"You're\", 9),\n",
       " ('feel', 9),\n",
       " ('match', 9),\n",
       " ('cold', 9),\n",
       " ('Dallas', 9),\n",
       " ('@ladygaga', 9),\n",
       " ('@carrieunderwood', 9),\n",
       " ('supposed', 9),\n",
       " ('LA', 9),\n",
       " ('NYC', 9),\n",
       " ('everything', 9),\n",
       " ('crew.', 9),\n",
       " ('service,', 9),\n",
       " ('Another', 9),\n",
       " ('9', 9),\n",
       " ('watch', 9),\n",
       " ('flight?', 9),\n",
       " ('possible', 9),\n",
       " ('under', 9),\n",
       " ('problems', 9),\n",
       " ('offered', 9),\n",
       " ('Austin', 9),\n",
       " ('money', 9),\n",
       " ('takes', 9),\n",
       " ('charged', 9),\n",
       " ('deal', 9),\n",
       " ('wanted', 9),\n",
       " ('round', 9),\n",
       " ('thanks!', 9),\n",
       " ('few', 9),\n",
       " ('currently', 9),\n",
       " ('child', 9),\n",
       " ('entertainment', 9),\n",
       " ('learn', 9),\n",
       " ('Did', 9),\n",
       " ('too.', 9),\n",
       " ('Delayed', 9),\n",
       " ('boarded', 9),\n",
       " ('drive', 9),\n",
       " ('ppl', 9),\n",
       " ('layover', 9),\n",
       " ('found', 9),\n",
       " ('thru', 9),\n",
       " ('Our', 9),\n",
       " ('seem', 9),\n",
       " ('point', 9),\n",
       " ('read', 9),\n",
       " ('bags.', 9),\n",
       " ('months', 9),\n",
       " ('probably', 9),\n",
       " ('frustrated', 9),\n",
       " ('seats.', 9),\n",
       " ('lounge', 9),\n",
       " ('load', 9),\n",
       " ('original', 9),\n",
       " ('taken', 9),\n",
       " (\"wouldn't\", 9),\n",
       " ('needed', 9),\n",
       " ('it?', 9),\n",
       " ('Great', 9),\n",
       " ('ORD.', 9),\n",
       " ('US', 9),\n",
       " ('25', 9),\n",
       " ('confirmed', 9),\n",
       " ('twice', 9),\n",
       " ('wife', 9),\n",
       " ('Maybe', 9),\n",
       " ('arrive', 9),\n",
       " ('sat', 9),\n",
       " ('runway', 9),\n",
       " ('THE', 9),\n",
       " ('added', 8),\n",
       " ('cool', 8),\n",
       " ('birthday', 8),\n",
       " ('during', 8),\n",
       " ('Are', 8),\n",
       " ('Club', 8),\n",
       " ('end', 8),\n",
       " ('NO', 8),\n",
       " ('not.', 8),\n",
       " ('hour.', 8),\n",
       " ('--', 8),\n",
       " ('Or', 8),\n",
       " ('lax', 8),\n",
       " ('must', 8),\n",
       " ('LAS', 8),\n",
       " ('provide', 8),\n",
       " ('Terrible', 8),\n",
       " ('error', 8),\n",
       " ('Only', 8),\n",
       " ('reply', 8),\n",
       " ('Who', 8),\n",
       " ('Got', 8),\n",
       " ('w', 8),\n",
       " ('quick', 8),\n",
       " ('now?', 8),\n",
       " ('access', 8),\n",
       " ('One', 8),\n",
       " ('Trying', 8),\n",
       " ('count', 8),\n",
       " ('check-in', 8),\n",
       " ('passenger', 8),\n",
       " (\"shouldn't\", 8),\n",
       " ('itinerary', 8),\n",
       " ('order', 8),\n",
       " ('expect', 8),\n",
       " (\"Let's\", 8),\n",
       " ('Looking', 8),\n",
       " ('glad', 8),\n",
       " ('pls', 8),\n",
       " ('request', 8),\n",
       " ('cost', 8),\n",
       " ('thought', 8),\n",
       " ('cannot', 8),\n",
       " ('address', 8),\n",
       " ('Also,', 8),\n",
       " ('#avgeek', 8),\n",
       " ('12', 8),\n",
       " ('Get', 8),\n",
       " ('Flighting', 8),\n",
       " ('helping', 8),\n",
       " ('As', 8),\n",
       " (\"there's\", 8),\n",
       " ('non', 8),\n",
       " ('yet.', 8),\n",
       " ('supervisor', 8),\n",
       " ('helpful.', 8),\n",
       " ('aircraft', 8),\n",
       " ('ok', 8),\n",
       " ('talking', 8),\n",
       " ('United.', 8),\n",
       " ('caused', 8),\n",
       " ('happens', 8),\n",
       " ('work.', 8),\n",
       " ('thanks,', 8),\n",
       " ('pretty', 8),\n",
       " ('TO', 8),\n",
       " ('1K', 8),\n",
       " ('counter', 8),\n",
       " ('seat.', 8),\n",
       " ('Twitter', 8),\n",
       " ('spent', 8),\n",
       " ('Premier', 8),\n",
       " ('bag,', 8),\n",
       " ('issue.', 8),\n",
       " ('awful', 8),\n",
       " ('tweet', 8),\n",
       " ('us.', 8),\n",
       " ('NEVER', 8),\n",
       " ('hate', 8),\n",
       " ('losing', 8),\n",
       " ('Yes', 8),\n",
       " ('attitude', 8),\n",
       " ('@staralliance', 8),\n",
       " ('domestic', 8),\n",
       " ('days.', 8),\n",
       " ('things', 7),\n",
       " ('together', 7),\n",
       " ('middle', 7),\n",
       " ('wish', 7),\n",
       " ('member', 7),\n",
       " ('four', 7),\n",
       " ('bring', 7),\n",
       " ('experience.', 7),\n",
       " (\"she's\", 7),\n",
       " ('card', 7),\n",
       " ('info.', 7),\n",
       " ('weather.', 7),\n",
       " ('Lots', 7),\n",
       " ('horrible', 7),\n",
       " ('&gt', 7),\n",
       " ('kids', 7),\n",
       " (\"We're\", 7),\n",
       " ('worse', 7),\n",
       " ('options', 7),\n",
       " ('Boston', 7),\n",
       " ('JFK', 7),\n",
       " ('DC', 7),\n",
       " ('iPhone', 7),\n",
       " ('done.', 7),\n",
       " ('I‚Äôm', 7),\n",
       " ('!', 7),\n",
       " ('now,', 7),\n",
       " ('route', 7),\n",
       " ('despite', 7),\n",
       " ('Newark.', 7),\n",
       " ('requested', 7),\n",
       " ('years', 7),\n",
       " ('fine.', 7),\n",
       " ('space', 7),\n",
       " ('loyal', 7),\n",
       " ('SFO.', 7),\n",
       " ('rental', 7),\n",
       " ('New', 7),\n",
       " ('For', 7),\n",
       " ('bought', 7),\n",
       " ('Been', 7),\n",
       " ('Really?', 7),\n",
       " ('it,', 7),\n",
       " ('nightmare', 7),\n",
       " ('MY', 7),\n",
       " ('whole', 7),\n",
       " ('complete', 7),\n",
       " ('customers.', 7),\n",
       " ('fees', 7),\n",
       " ('resolve', 7),\n",
       " ('..', 7),\n",
       " ('Then', 7),\n",
       " ('transfer', 7),\n",
       " ('explain', 7),\n",
       " ('pull', 7),\n",
       " ('guess', 7),\n",
       " ('else', 7),\n",
       " ('Airlines', 7),\n",
       " ('equipment', 7),\n",
       " ('Houston.', 7),\n",
       " ('=', 7),\n",
       " (\"you'll\", 7),\n",
       " ('economy', 7),\n",
       " ('around', 7),\n",
       " ('better.', 7),\n",
       " ('time?', 7),\n",
       " ('ready', 7),\n",
       " ('1k', 7),\n",
       " ('communication', 7),\n",
       " ('$', 7),\n",
       " ('usually', 7),\n",
       " ('extremely', 7),\n",
       " ('clothes', 7),\n",
       " ('leg', 7),\n",
       " ('much.', 7),\n",
       " ('#unitedairlines', 7),\n",
       " ('delivered', 7),\n",
       " ('held', 7),\n",
       " ('offering', 7),\n",
       " ('arriving', 7),\n",
       " ('ever!', 7),\n",
       " ('assistance.', 7),\n",
       " ('members', 7),\n",
       " ('#United', 7),\n",
       " ('post', 7),\n",
       " ('cust', 7),\n",
       " ('literally', 7),\n",
       " ('final', 7),\n",
       " ('Chicago.', 7),\n",
       " ('offer', 7),\n",
       " ('landed.', 7),\n",
       " ('DEN', 7),\n",
       " ('claim.', 7),\n",
       " ('denied', 7),\n",
       " ('course', 7),\n",
       " ('changes', 7),\n",
       " ('angry', 7),\n",
       " ('Would', 7),\n",
       " (\"aren't\", 7),\n",
       " ('fill', 7),\n",
       " ('delayed.', 7),\n",
       " ('pick', 7),\n",
       " ('connection.', 7),\n",
       " ('suck.', 7),\n",
       " ('agent.', 7),\n",
       " ('pilots', 7),\n",
       " ('following', 7),\n",
       " ('no,', 7),\n",
       " ('VX', 6),\n",
       " ('early.', 6),\n",
       " ('country', 6),\n",
       " ('seating', 6),\n",
       " ('friends', 6),\n",
       " ('response.', 6),\n",
       " (\"we'll\", 6),\n",
       " ('red', 6),\n",
       " ('you,', 6),\n",
       " ('cause', 6),\n",
       " ('expensive', 6),\n",
       " ('Help?', 6),\n",
       " ('DCA', 6),\n",
       " ('Hi,', 6),\n",
       " ('Let', 6),\n",
       " ('all.', 6),\n",
       " ('service?', 6),\n",
       " ('code', 6),\n",
       " ('down?', 6),\n",
       " ('phone.', 6),\n",
       " ('deals', 6),\n",
       " ('cities', 6),\n",
       " ('2015', 6),\n",
       " ('totally', 6),\n",
       " ('depart', 6),\n",
       " ('#frustrated', 6),\n",
       " ('paying', 6),\n",
       " ('answer', 6),\n",
       " ('closed', 6),\n",
       " ('assist', 6),\n",
       " ('Baggage', 6),\n",
       " ('r', 6),\n",
       " ('BOS', 6),\n",
       " ('nothing.', 6),\n",
       " ('stuff', 6),\n",
       " ('apparently', 6),\n",
       " ('Keep', 6),\n",
       " ('problems.', 6),\n",
       " ('delayed,', 6),\n",
       " ('üëè', 6),\n",
       " ('YOU', 6),\n",
       " ('fun', 6),\n",
       " ('updates', 6),\n",
       " ('today,', 6),\n",
       " ...]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's count words\n",
    "\n",
    "big_list1 = []\n",
    "\n",
    "for i in range(len(df_train_core_without_nan)):\n",
    "    each_list = df_train_core_without_nan.text[i].split()\n",
    "    big_list1 = big_list1 + each_list\n",
    "\n",
    "word_count = Counter(big_list1)\n",
    "word_count_most_common = word_count.most_common()\n",
    "word_count_most_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2708 362 498 1848 0.6824224519940916\n"
     ]
    }
   ],
   "source": [
    "#so this shows @united is a quite negative word.  P(negative | '@united') = 0.6824\n",
    "\n",
    "total_count = 0 \n",
    "positive_count = 0 \n",
    "neutral_count = 0 \n",
    "negative_count = 0 \n",
    "\n",
    "for i in range(len(df_train_core_without_nan)):\n",
    "    list1 = df_train_core_without_nan.text[i].split()\n",
    "    if '@united' in list1:\n",
    "        total_count = total_count + 1\n",
    "    if '@united' in list1 and df_train_core_without_nan.airline_sentiment[i] == 'positive':\n",
    "        positive_count = positive_count + 1\n",
    "    if '@united' in list1 and df_train_core_without_nan.airline_sentiment[i] == 'neutral':\n",
    "        neutral_count = neutral_count + 1\n",
    "    if '@united' in list1 and df_train_core_without_nan.airline_sentiment[i] == 'negative':\n",
    "        negative_count = negative_count + 1\n",
    "print(total_count, positive_count, neutral_count, negative_count, negative_count/total_count)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the dataframe for all words\n",
    "\n",
    "big_list = []\n",
    "\n",
    "for j in range(len(word_count_most_common)):\n",
    "    \n",
    "    total_count = 0 \n",
    "    positive_count = 0 \n",
    "    neutral_count = 0 \n",
    "    negative_count = 0 \n",
    "    word = word_count_most_common[j][0]\n",
    "    row = []\n",
    "    \n",
    "    for i in range(len(df_train_core_without_nan)):\n",
    "        list1 = df_train_core_without_nan.text[i].split()\n",
    "        if word in list1:\n",
    "            total_count = total_count + 1\n",
    "        if word in list1 and df_train_core_without_nan.airline_sentiment[i] == 'positive':\n",
    "            positive_count = positive_count + 1\n",
    "        if word in list1 and df_train_core_without_nan.airline_sentiment[i] == 'neutral':\n",
    "            neutral_count = neutral_count + 1\n",
    "        if word in list1 and df_train_core_without_nan.airline_sentiment[i] == 'negative':\n",
    "            negative_count = negative_count + 1\n",
    "    row.append(word)\n",
    "    row.append(total_count)\n",
    "    row.append(positive_count)\n",
    "    row.append(neutral_count)\n",
    "    row.append(negative_count)\n",
    "    row.append(positive_count/total_count)\n",
    "    row.append(neutral_count/total_count)\n",
    "    row.append(negative_count/total_count)\n",
    "    \n",
    "    big_list.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>total_count</th>\n",
       "      <th>positive_count</th>\n",
       "      <th>neutral_count</th>\n",
       "      <th>negative_count</th>\n",
       "      <th>pos_prob</th>\n",
       "      <th>neu_prob</th>\n",
       "      <th>neg_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@united</td>\n",
       "      <td>2708</td>\n",
       "      <td>362</td>\n",
       "      <td>498</td>\n",
       "      <td>1848</td>\n",
       "      <td>0.133678</td>\n",
       "      <td>0.183900</td>\n",
       "      <td>0.682422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>to</td>\n",
       "      <td>1382</td>\n",
       "      <td>168</td>\n",
       "      <td>258</td>\n",
       "      <td>956</td>\n",
       "      <td>0.121563</td>\n",
       "      <td>0.186686</td>\n",
       "      <td>0.691751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the</td>\n",
       "      <td>1056</td>\n",
       "      <td>169</td>\n",
       "      <td>159</td>\n",
       "      <td>728</td>\n",
       "      <td>0.160038</td>\n",
       "      <td>0.150568</td>\n",
       "      <td>0.689394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I</td>\n",
       "      <td>854</td>\n",
       "      <td>106</td>\n",
       "      <td>176</td>\n",
       "      <td>572</td>\n",
       "      <td>0.124122</td>\n",
       "      <td>0.206089</td>\n",
       "      <td>0.669789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a</td>\n",
       "      <td>866</td>\n",
       "      <td>88</td>\n",
       "      <td>142</td>\n",
       "      <td>636</td>\n",
       "      <td>0.101617</td>\n",
       "      <td>0.163972</td>\n",
       "      <td>0.734411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>for</td>\n",
       "      <td>773</td>\n",
       "      <td>142</td>\n",
       "      <td>129</td>\n",
       "      <td>502</td>\n",
       "      <td>0.183700</td>\n",
       "      <td>0.166882</td>\n",
       "      <td>0.649418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>you</td>\n",
       "      <td>676</td>\n",
       "      <td>99</td>\n",
       "      <td>136</td>\n",
       "      <td>441</td>\n",
       "      <td>0.146450</td>\n",
       "      <td>0.201183</td>\n",
       "      <td>0.652367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>and</td>\n",
       "      <td>677</td>\n",
       "      <td>87</td>\n",
       "      <td>68</td>\n",
       "      <td>522</td>\n",
       "      <td>0.128508</td>\n",
       "      <td>0.100443</td>\n",
       "      <td>0.771049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>on</td>\n",
       "      <td>674</td>\n",
       "      <td>68</td>\n",
       "      <td>114</td>\n",
       "      <td>492</td>\n",
       "      <td>0.100890</td>\n",
       "      <td>0.169139</td>\n",
       "      <td>0.729970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>my</td>\n",
       "      <td>606</td>\n",
       "      <td>69</td>\n",
       "      <td>85</td>\n",
       "      <td>452</td>\n",
       "      <td>0.113861</td>\n",
       "      <td>0.140264</td>\n",
       "      <td>0.745875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>is</td>\n",
       "      <td>541</td>\n",
       "      <td>49</td>\n",
       "      <td>82</td>\n",
       "      <td>410</td>\n",
       "      <td>0.090573</td>\n",
       "      <td>0.151571</td>\n",
       "      <td>0.757856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>in</td>\n",
       "      <td>520</td>\n",
       "      <td>51</td>\n",
       "      <td>91</td>\n",
       "      <td>378</td>\n",
       "      <td>0.098077</td>\n",
       "      <td>0.175000</td>\n",
       "      <td>0.726923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>flight</td>\n",
       "      <td>511</td>\n",
       "      <td>53</td>\n",
       "      <td>79</td>\n",
       "      <td>379</td>\n",
       "      <td>0.103718</td>\n",
       "      <td>0.154599</td>\n",
       "      <td>0.741683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>of</td>\n",
       "      <td>443</td>\n",
       "      <td>56</td>\n",
       "      <td>63</td>\n",
       "      <td>324</td>\n",
       "      <td>0.126411</td>\n",
       "      <td>0.142212</td>\n",
       "      <td>0.731377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>@VirginAmerica</td>\n",
       "      <td>423</td>\n",
       "      <td>129</td>\n",
       "      <td>136</td>\n",
       "      <td>158</td>\n",
       "      <td>0.304965</td>\n",
       "      <td>0.321513</td>\n",
       "      <td>0.373522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>your</td>\n",
       "      <td>337</td>\n",
       "      <td>38</td>\n",
       "      <td>34</td>\n",
       "      <td>265</td>\n",
       "      <td>0.112760</td>\n",
       "      <td>0.100890</td>\n",
       "      <td>0.786350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>have</td>\n",
       "      <td>346</td>\n",
       "      <td>34</td>\n",
       "      <td>62</td>\n",
       "      <td>250</td>\n",
       "      <td>0.098266</td>\n",
       "      <td>0.179191</td>\n",
       "      <td>0.722543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>with</td>\n",
       "      <td>338</td>\n",
       "      <td>46</td>\n",
       "      <td>62</td>\n",
       "      <td>230</td>\n",
       "      <td>0.136095</td>\n",
       "      <td>0.183432</td>\n",
       "      <td>0.680473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>me</td>\n",
       "      <td>319</td>\n",
       "      <td>43</td>\n",
       "      <td>58</td>\n",
       "      <td>218</td>\n",
       "      <td>0.134796</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.683386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>it</td>\n",
       "      <td>298</td>\n",
       "      <td>45</td>\n",
       "      <td>54</td>\n",
       "      <td>199</td>\n",
       "      <td>0.151007</td>\n",
       "      <td>0.181208</td>\n",
       "      <td>0.667785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>was</td>\n",
       "      <td>303</td>\n",
       "      <td>41</td>\n",
       "      <td>29</td>\n",
       "      <td>233</td>\n",
       "      <td>0.135314</td>\n",
       "      <td>0.095710</td>\n",
       "      <td>0.768977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>at</td>\n",
       "      <td>298</td>\n",
       "      <td>27</td>\n",
       "      <td>41</td>\n",
       "      <td>230</td>\n",
       "      <td>0.090604</td>\n",
       "      <td>0.137584</td>\n",
       "      <td>0.771812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>that</td>\n",
       "      <td>286</td>\n",
       "      <td>21</td>\n",
       "      <td>32</td>\n",
       "      <td>233</td>\n",
       "      <td>0.073427</td>\n",
       "      <td>0.111888</td>\n",
       "      <td>0.814685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>not</td>\n",
       "      <td>277</td>\n",
       "      <td>16</td>\n",
       "      <td>25</td>\n",
       "      <td>236</td>\n",
       "      <td>0.057762</td>\n",
       "      <td>0.090253</td>\n",
       "      <td>0.851986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>from</td>\n",
       "      <td>278</td>\n",
       "      <td>21</td>\n",
       "      <td>78</td>\n",
       "      <td>179</td>\n",
       "      <td>0.075540</td>\n",
       "      <td>0.280576</td>\n",
       "      <td>0.643885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>get</td>\n",
       "      <td>247</td>\n",
       "      <td>23</td>\n",
       "      <td>45</td>\n",
       "      <td>179</td>\n",
       "      <td>0.093117</td>\n",
       "      <td>0.182186</td>\n",
       "      <td>0.724696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>be</td>\n",
       "      <td>244</td>\n",
       "      <td>27</td>\n",
       "      <td>44</td>\n",
       "      <td>173</td>\n",
       "      <td>0.110656</td>\n",
       "      <td>0.180328</td>\n",
       "      <td>0.709016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>but</td>\n",
       "      <td>236</td>\n",
       "      <td>23</td>\n",
       "      <td>30</td>\n",
       "      <td>183</td>\n",
       "      <td>0.097458</td>\n",
       "      <td>0.127119</td>\n",
       "      <td>0.775424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>no</td>\n",
       "      <td>228</td>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "      <td>206</td>\n",
       "      <td>0.017544</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>0.903509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>this</td>\n",
       "      <td>218</td>\n",
       "      <td>34</td>\n",
       "      <td>34</td>\n",
       "      <td>150</td>\n",
       "      <td>0.155963</td>\n",
       "      <td>0.155963</td>\n",
       "      <td>0.688073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10881</th>\n",
       "      <td>details-no</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10882</th>\n",
       "      <td>began</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10883</th>\n",
       "      <td>#UA507</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10884</th>\n",
       "      <td>Tks</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10885</th>\n",
       "      <td>POTUS</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10886</th>\n",
       "      <td>DVR</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10887</th>\n",
       "      <td>motion</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10888</th>\n",
       "      <td>sick.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10889</th>\n",
       "      <td>http://t.co/m81rV0blxs</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10890</th>\n",
       "      <td>-pilots,</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10891</th>\n",
       "      <td>personnel</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10892</th>\n",
       "      <td>job.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10893</th>\n",
       "      <td>blame...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10894</th>\n",
       "      <td>http://t.co/LoLuWfCi11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10895</th>\n",
       "      <td>\"fix\"</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10896</th>\n",
       "      <td>drinking</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10897</th>\n",
       "      <td>yourselves</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10898</th>\n",
       "      <td>overpriced</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10899</th>\n",
       "      <td>sub</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10900</th>\n",
       "      <td>par</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10901</th>\n",
       "      <td>idea:</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10902</th>\n",
       "      <td>overflowed</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10903</th>\n",
       "      <td>toilet.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10904</th>\n",
       "      <td>plumber</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10905</th>\n",
       "      <td>luxurious</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10906</th>\n",
       "      <td>explains</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10907</th>\n",
       "      <td>6:18</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10908</th>\n",
       "      <td>6:07</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10909</th>\n",
       "      <td>trace</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10910</th>\n",
       "      <td>Albuquerque</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10911 rows √ó 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         word  total_count  positive_count  neutral_count  \\\n",
       "0                     @united         2708             362            498   \n",
       "1                          to         1382             168            258   \n",
       "2                         the         1056             169            159   \n",
       "3                           I          854             106            176   \n",
       "4                           a          866              88            142   \n",
       "5                         for          773             142            129   \n",
       "6                         you          676              99            136   \n",
       "7                         and          677              87             68   \n",
       "8                          on          674              68            114   \n",
       "9                          my          606              69             85   \n",
       "10                         is          541              49             82   \n",
       "11                         in          520              51             91   \n",
       "12                     flight          511              53             79   \n",
       "13                         of          443              56             63   \n",
       "14             @VirginAmerica          423             129            136   \n",
       "15                       your          337              38             34   \n",
       "16                       have          346              34             62   \n",
       "17                       with          338              46             62   \n",
       "18                         me          319              43             58   \n",
       "19                         it          298              45             54   \n",
       "20                        was          303              41             29   \n",
       "21                         at          298              27             41   \n",
       "22                       that          286              21             32   \n",
       "23                        not          277              16             25   \n",
       "24                       from          278              21             78   \n",
       "25                        get          247              23             45   \n",
       "26                         be          244              27             44   \n",
       "27                        but          236              23             30   \n",
       "28                         no          228               4             18   \n",
       "29                       this          218              34             34   \n",
       "...                       ...          ...             ...            ...   \n",
       "10881              details-no            1               0              0   \n",
       "10882                   began            1               0              0   \n",
       "10883                  #UA507            1               0              0   \n",
       "10884                     Tks            1               0              0   \n",
       "10885                   POTUS            1               0              0   \n",
       "10886                     DVR            1               0              0   \n",
       "10887                  motion            1               0              0   \n",
       "10888                   sick.            1               0              0   \n",
       "10889  http://t.co/m81rV0blxs            1               0              0   \n",
       "10890                -pilots,            1               1              0   \n",
       "10891               personnel            1               1              0   \n",
       "10892                    job.            1               1              0   \n",
       "10893                blame...            1               1              0   \n",
       "10894  http://t.co/LoLuWfCi11            1               0              0   \n",
       "10895                   \"fix\"            1               0              0   \n",
       "10896                drinking            1               0              1   \n",
       "10897              yourselves            1               0              0   \n",
       "10898              overpriced            1               0              0   \n",
       "10899                     sub            1               0              0   \n",
       "10900                     par            1               0              0   \n",
       "10901                   idea:            1               0              0   \n",
       "10902              overflowed            1               0              0   \n",
       "10903                 toilet.            1               0              0   \n",
       "10904                 plumber            1               0              0   \n",
       "10905               luxurious            1               0              0   \n",
       "10906                explains            1               0              0   \n",
       "10907                    6:18            1               0              0   \n",
       "10908                    6:07            1               0              0   \n",
       "10909                   trace            1               0              0   \n",
       "10910             Albuquerque            1               0              0   \n",
       "\n",
       "       negative_count  pos_prob  neu_prob  neg_prob  \n",
       "0                1848  0.133678  0.183900  0.682422  \n",
       "1                 956  0.121563  0.186686  0.691751  \n",
       "2                 728  0.160038  0.150568  0.689394  \n",
       "3                 572  0.124122  0.206089  0.669789  \n",
       "4                 636  0.101617  0.163972  0.734411  \n",
       "5                 502  0.183700  0.166882  0.649418  \n",
       "6                 441  0.146450  0.201183  0.652367  \n",
       "7                 522  0.128508  0.100443  0.771049  \n",
       "8                 492  0.100890  0.169139  0.729970  \n",
       "9                 452  0.113861  0.140264  0.745875  \n",
       "10                410  0.090573  0.151571  0.757856  \n",
       "11                378  0.098077  0.175000  0.726923  \n",
       "12                379  0.103718  0.154599  0.741683  \n",
       "13                324  0.126411  0.142212  0.731377  \n",
       "14                158  0.304965  0.321513  0.373522  \n",
       "15                265  0.112760  0.100890  0.786350  \n",
       "16                250  0.098266  0.179191  0.722543  \n",
       "17                230  0.136095  0.183432  0.680473  \n",
       "18                218  0.134796  0.181818  0.683386  \n",
       "19                199  0.151007  0.181208  0.667785  \n",
       "20                233  0.135314  0.095710  0.768977  \n",
       "21                230  0.090604  0.137584  0.771812  \n",
       "22                233  0.073427  0.111888  0.814685  \n",
       "23                236  0.057762  0.090253  0.851986  \n",
       "24                179  0.075540  0.280576  0.643885  \n",
       "25                179  0.093117  0.182186  0.724696  \n",
       "26                173  0.110656  0.180328  0.709016  \n",
       "27                183  0.097458  0.127119  0.775424  \n",
       "28                206  0.017544  0.078947  0.903509  \n",
       "29                150  0.155963  0.155963  0.688073  \n",
       "...               ...       ...       ...       ...  \n",
       "10881               1  0.000000  0.000000  1.000000  \n",
       "10882               1  0.000000  0.000000  1.000000  \n",
       "10883               1  0.000000  0.000000  1.000000  \n",
       "10884               1  0.000000  0.000000  1.000000  \n",
       "10885               1  0.000000  0.000000  1.000000  \n",
       "10886               1  0.000000  0.000000  1.000000  \n",
       "10887               1  0.000000  0.000000  1.000000  \n",
       "10888               1  0.000000  0.000000  1.000000  \n",
       "10889               1  0.000000  0.000000  1.000000  \n",
       "10890               0  1.000000  0.000000  0.000000  \n",
       "10891               0  1.000000  0.000000  0.000000  \n",
       "10892               0  1.000000  0.000000  0.000000  \n",
       "10893               0  1.000000  0.000000  0.000000  \n",
       "10894               1  0.000000  0.000000  1.000000  \n",
       "10895               1  0.000000  0.000000  1.000000  \n",
       "10896               0  0.000000  1.000000  0.000000  \n",
       "10897               1  0.000000  0.000000  1.000000  \n",
       "10898               1  0.000000  0.000000  1.000000  \n",
       "10899               1  0.000000  0.000000  1.000000  \n",
       "10900               1  0.000000  0.000000  1.000000  \n",
       "10901               1  0.000000  0.000000  1.000000  \n",
       "10902               1  0.000000  0.000000  1.000000  \n",
       "10903               1  0.000000  0.000000  1.000000  \n",
       "10904               1  0.000000  0.000000  1.000000  \n",
       "10905               1  0.000000  0.000000  1.000000  \n",
       "10906               1  0.000000  0.000000  1.000000  \n",
       "10907               1  0.000000  0.000000  1.000000  \n",
       "10908               1  0.000000  0.000000  1.000000  \n",
       "10909               1  0.000000  0.000000  1.000000  \n",
       "10910               1  0.000000  0.000000  1.000000  \n",
       "\n",
       "[10911 rows x 8 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = ['word', 'total_count', 'positive_count', 'neutral_count', 'negative_count', 'pos_prob', 'neu_prob', 'neg_prob']\n",
    "df_train_analyzed = pd.DataFrame.from_records(big_list, columns = labels)\n",
    "df_train_analyzed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>total_count</th>\n",
       "      <th>positive_count</th>\n",
       "      <th>neutral_count</th>\n",
       "      <th>negative_count</th>\n",
       "      <th>pos_prob</th>\n",
       "      <th>neu_prob</th>\n",
       "      <th>neg_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>worst</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      word  total_count  positive_count  neutral_count  negative_count  \\\n",
       "205  worst           39               0              0              39   \n",
       "\n",
       "     pos_prob  neu_prob  neg_prob  \n",
       "205       0.0       0.0       1.0  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so if you look at the word \"worst\", probability that the sentence with 'worst' in it is negative is 1.0. \n",
    "\n",
    "df_train_analyzed[df_train_analyzed.word == 'worst']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def biggest(x, y, z):\n",
    "    \n",
    "    Max = x\n",
    "    status = 'positive'\n",
    "    if y > Max:\n",
    "        Max = y   \n",
    "        status = 'neutral'\n",
    "    if z > Max:\n",
    "        Max = z\n",
    "        status = 'negative'\n",
    "        if y > z:\n",
    "            Max = y\n",
    "            status = 'neutral'\n",
    "    return Max, status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is  0.9433792690233673\n"
     ]
    }
   ],
   "source": [
    "# now, we look at each sentence.  we take the higest probability from each word, and label the sentence with the corresponding label.  \n",
    "# for example, if we spot a word 'worst', in the sentence, then the sentence will be labeled as 'negative', p(negative|worst) = 1\n",
    "# and we compute the accuracy for whole training set.  \n",
    "df_train_analyzed_rescaled = df_train_analyzed.copy()\n",
    "\n",
    "\n",
    "c = 0 \n",
    "\n",
    "for j in range(len(df_train_core_without_nan)):\n",
    "\n",
    "    list2 = df_train_core_without_nan.text[j].split()\n",
    "    list2_to_pd = []\n",
    "    df_sentence = []\n",
    "\n",
    "    for i in range(len(list2)):\n",
    "        word2 = list2[i]\n",
    "        a = df_train_analyzed_rescaled.loc[df_train_analyzed_rescaled['word'] == word2]\n",
    "        b = a[['word', 'pos_prob', 'neu_prob', 'neg_prob']].values.tolist()\n",
    "        list2_to_pd.append(b[0])\n",
    "\n",
    "    df_sentence = pd.DataFrame(list2_to_pd, columns =['word', 'pos_prob', 'neu_prob', 'neg_prob'] )\n",
    "    k = biggest(df_sentence['pos_prob'].max(), df_sentence['neu_prob'].max(), df_sentence['neg_prob'].max())\n",
    "    if k[1] == df_train_core_without_nan.airline_sentiment[j]:\n",
    "        c = c + 1\n",
    "        \n",
    "print('accuracy is ', c/len(df_train_core_without_nan))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>total_count</th>\n",
       "      <th>positive_count</th>\n",
       "      <th>neutral_count</th>\n",
       "      <th>negative_count</th>\n",
       "      <th>pos_prob</th>\n",
       "      <th>neu_prob</th>\n",
       "      <th>neg_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>thank</td>\n",
       "      <td>67</td>\n",
       "      <td>50</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>0.746269</td>\n",
       "      <td>0.104478</td>\n",
       "      <td>0.149254</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     word  total_count  positive_count  neutral_count  negative_count  \\\n",
       "99  thank           67              50              7              10   \n",
       "\n",
       "    pos_prob  neu_prob  neg_prob  \n",
       "99  0.746269  0.104478  0.149254  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# it turns out accuracy is 94%, which is good.  But we see generally negative probability are higher than positive or neutral.  \n",
    "# For instance, if you look at the word 'thank', p(positive | thank) = 0.746\n",
    "# If you look at the word 'thank' in the sentence, \n",
    "# it is likely, the sentence is positive, but 0.746 is not as definite as negative words like 'worst' which gives neg prob 1. \n",
    "# this is because most of tweet are complains.  \n",
    "# people writes complains, but usually most people don't bother to write good compliments.  \n",
    "# so neg_prob are more buffed than pos_prob.  \n",
    "# So we rescale these probability to see if we can improve accuracy.  \n",
    "\n",
    "df_train_analyzed[df_train_analyzed.word == 'thank']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The trick is power by 1/2 on pos_prob.  This will increse pos_prob, but keep it under 1. \n",
    "# power by 1/4 on neu_prob.  This will increase neu_prob, but keep it under 1.  \n",
    "# Power by 2 on neg_prob.  This will decrease neg_prob, but keep is under 1.  \n",
    "# notice if the prob is exactly 1, the powering doesn't change its value.  it still remains at 1.  \n",
    "\n",
    "df_train_analyzed_rescaled = df_train_analyzed.copy()\n",
    "\n",
    "e = 2\n",
    "\n",
    "df_train_analyzed_rescaled['pos_prob'] = df_train_analyzed_rescaled['pos_prob'].apply(lambda x : pow(x, 1/e))\n",
    "df_train_analyzed_rescaled['neu_prob'] = df_train_analyzed_rescaled['neu_prob'].apply(lambda x : pow(x, 1/(2*e )))\n",
    "df_train_analyzed_rescaled['neg_prob'] = df_train_analyzed_rescaled['neg_prob'].apply(lambda x : pow(x, e ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is  0.9766327142001199\n"
     ]
    }
   ],
   "source": [
    "# you might ask why e = 2.  this number came out from trial and error.  I tried different integers and accuracy was the highest. \n",
    "\n",
    "c = 0 \n",
    "\n",
    "for j in range(len(df_train_core_without_nan)):\n",
    "\n",
    "    list2 = df_train_core_without_nan.text[j].split()\n",
    "    list2_to_pd = []\n",
    "    df_sentence = []\n",
    "\n",
    "    for i in range(len(list2)):\n",
    "        word2 = list2[i]\n",
    "        a = df_train_analyzed_rescaled.loc[df_train_analyzed_rescaled['word'] == word2]\n",
    "        b = a[['word', 'pos_prob', 'neu_prob', 'neg_prob']].values.tolist()\n",
    "        list2_to_pd.append(b[0])\n",
    "\n",
    "    df_sentence = pd.DataFrame(list2_to_pd, columns =['word', 'pos_prob', 'neu_prob', 'neg_prob'] )\n",
    "    k = biggest(df_sentence['pos_prob'].max(), df_sentence['neu_prob'].max(), df_sentence['neg_prob'].max())\n",
    "    if k[1] == df_train_core_without_nan.airline_sentiment[j]:\n",
    "        c = c + 1\n",
    "        \n",
    "print('accuracy is ', c/len(df_train_core_without_nan))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy is 0.9766, which is good.  \n",
    "# we can run same thing to test set, and submit.  \n",
    "# But here is the problem, what if there is the word in test set that has never appeared in training set.  \n",
    "# we know conditional probablities on each word in training set, but not in every words in test set.  \n",
    "# we need solve this issue, otherwise it will show error. \n",
    "# so we assign probabilities to word not in training set, with 0.  \n",
    "\n",
    "\n",
    "df_test_core = df_test['text']\n",
    "big_list2 = []\n",
    "\n",
    "for i in range(len(df_test_core)):\n",
    "    each_list = df_test_core[i].split()\n",
    "    big_list2 = big_list2 + each_list\n",
    "\n",
    "word_count = Counter(big_list2)\n",
    "word_count_most_common = word_count.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = []\n",
    "\n",
    "for i in range(824):\n",
    "    index.append(10911 + i)\n",
    "\n",
    "labels = ['word', 'total_count', 'positive_count', 'neutral_count', 'negative_count', 'pos_prob', 'neu_prob', 'neg_prob']\n",
    "big_list = []\n",
    "s = set(df_train_analyzed['word'])\n",
    "c = 0 \n",
    "for i in range(len(word_count_most_common)):\n",
    "    if word_count_most_common[i][0] not in s:\n",
    "        list3 = []\n",
    "        list3.append(word_count_most_common[i][0])\n",
    "        for j in range(7):\n",
    "            list3.append(0.0)\n",
    "        big_list.append(list3)\n",
    "        \n",
    "df_big_list = pd.DataFrame(big_list, columns = labels, index = index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_test_combined = pd.concat([df_train_analyzed_rescaled, df_big_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now this produces the submission csv file for test set.  \n",
    "\n",
    "c = 0 \n",
    "labels = ['tweet_id', 'airline_sentiment']\n",
    "big_result = []\n",
    "\n",
    "for j in range(len(df_test_core)):\n",
    "\n",
    "    list2 = df_test_core[j].split()\n",
    "    list2_to_pd = []\n",
    "    df_sentence = []\n",
    "    result = []   \n",
    "\n",
    "    for i in range(len(list2)):\n",
    "        word2 = list2[i]\n",
    "        a = df_train_test_combined.loc[df_train_test_combined['word'] == word2]\n",
    "        b = a[['word', 'pos_prob', 'neu_prob', 'neg_prob']].values.tolist()\n",
    "        list2_to_pd.append(b[0])\n",
    "\n",
    "    df_sentence = pd.DataFrame(list2_to_pd, columns =['word', 'pos_prob', 'neu_prob', 'neg_prob'] )\n",
    "    k = biggest(df_sentence['pos_prob'].max(), df_sentence['neu_prob'].max(), df_sentence['neg_prob'].max())\n",
    "    \n",
    "    result.append(df_test.tweet_id[j])\n",
    "    result.append(k[1])\n",
    "\n",
    "    big_result.append(result)\n",
    "\n",
    "df_submission = pd.DataFrame(big_result, columns = labels)\n",
    "\n",
    "df_submission.to_csv('output1.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# But we haven't done any machine learning yet.  I just took the highest probability and labeled the sentence according to it.  \n",
    "# If we run any other machine learning algorithm, such as random forrest, svm, \n",
    "# neural network, etc.. then the result will be better?  I mean, the inputs are just three numbers, we just need to classify.  \n",
    "# Let's find out. \n",
    "# First, let's change dataframe to numpy array, so we can run machine learning.  \n",
    "\n",
    "airline_sentiment_numeric = []\n",
    "\n",
    "for i in range(len(df_train_core_without_nan)):\n",
    "    if df_train_core_without_nan.airline_sentiment[i] == 'negative':\n",
    "        airline_sentiment_numeric.append(-1)\n",
    "    if df_train_core_without_nan.airline_sentiment[i] == 'neutral':\n",
    "        airline_sentiment_numeric.append(0)\n",
    "    else:\n",
    "        airline_sentiment_numeric.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_core_without_nan_and_numeric = df_train_core_without_nan.copy()\n",
    "\n",
    "df_train_core_without_nan_and_numeric['airline_sentiment'] = df_train_core_without_nan_and_numeric['airline_sentiment'].apply(\n",
    "{'negative':-1, 'neutral':0, 'positive': 1}.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0 \n",
    "\n",
    "train_images = []\n",
    "train_labels = []\n",
    "\n",
    "for j in range(len(df_train_core_without_nan_and_numeric)):\n",
    "\n",
    "    list2 = df_train_core_without_nan_and_numeric.text[j].split()\n",
    "    list2_to_pd = []\n",
    "    df_sentence = []\n",
    "    list3 = []\n",
    "\n",
    "    for i in range(len(list2)):\n",
    "        word2 = list2[i]\n",
    "        a = df_train_analyzed_rescaled.loc[df_train_analyzed_rescaled['word'] == word2]\n",
    "        b = a[['word', 'pos_prob', 'neu_prob', 'neg_prob']].values.tolist()\n",
    "        list2_to_pd.append(b[0])\n",
    "\n",
    "    df_sentence = pd.DataFrame(list2_to_pd, columns =['word', 'pos_prob', 'neu_prob', 'neg_prob'] )\n",
    "    list3.append(df_sentence['pos_prob'].max())\n",
    "    list3.append(df_sentence['neu_prob'].max())\n",
    "    list3.append(df_sentence['neg_prob'].max())\n",
    "    \n",
    "    train_images.append(list3)\n",
    "    train_labels.append(df_train_core_without_nan_and_numeric.airline_sentiment[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hongj\\Anaconda2\\envs\\Tensorflow\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "df_train_images = pd.DataFrame(train_images)\n",
    "train_images_to_array = df_train_images.as_matrix(columns=None).astype(np.float)\n",
    "\n",
    "df_train_labels = pd.DataFrame(train_labels)\n",
    "train_labels_to_array = np.array(df_train_labels[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_core = pd.DataFrame(df_test_core)\n",
    "\n",
    "c = 0 \n",
    "\n",
    "test_images = []\n",
    "\n",
    "for j in range(len(df_test_core)):\n",
    "\n",
    "    list2 = df_test_core.text[j].split()\n",
    "    list2_to_pd = []\n",
    "    df_sentence = []\n",
    "    list3 = []\n",
    "\n",
    "    for i in range(len(list2)):\n",
    "        word2 = list2[i]\n",
    "        a = df_train_test_combined.loc[df_train_test_combined['word'] == word2]\n",
    "        b = a[['word', 'pos_prob', 'neu_prob', 'neg_prob']].values.tolist()\n",
    "        list2_to_pd.append(b[0])\n",
    "\n",
    "    df_sentence = pd.DataFrame(list2_to_pd, columns =['word', 'pos_prob', 'neu_prob', 'neg_prob'] )\n",
    "    list3.append(df_sentence['pos_prob'].max())\n",
    "    list3.append(df_sentence['neu_prob'].max())\n",
    "    list3.append(df_sentence['neg_prob'].max())\n",
    "    \n",
    "    test_images.append(list3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hongj\\Anaconda2\\envs\\Tensorflow\\lib\\site-packages\\ipykernel_launcher.py:3: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "test_images = pd.DataFrame(test_images)\n",
    "\n",
    "test_images_to_array = test_images.as_matrix(columns=None).astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split( train_images_to_array, train_labels_to_array,\n",
    "                                                    test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 668 points : 10\n",
      "Accuracy is  0.9850299401197605\n"
     ]
    }
   ],
   "source": [
    "# Ok now data sets are ready.  We can run different machine learning.  We will naive baysian first.  \n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "clf = GaussianNB()\n",
    "y_pred = clf.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (X_test.shape[0],(y_test != y_pred).sum()))\n",
    "print(\"Accuracy is \",  (y_test == y_pred).sum() / X_test.shape[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy is impressive 0.985.  Let's make the submission set. \n",
    "\n",
    "col1 = df_test['tweet_id'].tolist()\n",
    "y_pred = clf.fit(X_train, y_train).predict(test_images_to_array)\n",
    "labels = ['tweet_id', 'airline_sentiment']\n",
    "d = {'airline_sentiment': y_pred, 'tweet_id': col1}\n",
    "df = pd.DataFrame(data=d)\n",
    "df = df.reindex(columns=labels)\n",
    "df['airline_sentiment'] = df['airline_sentiment'].apply({-1 : 'negative', 0:'neutral', 1:'positive' }.get)\n",
    "\n",
    "df.to_csv('outputNB.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 668 points : 13\n",
      "Accuracy is  0.9805389221556886\n"
     ]
    }
   ],
   "source": [
    "# Let's try KNN \n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "clf = KNeighborsClassifier(n_neighbors=3)\n",
    "y_pred = clf.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (X_test.shape[0],(y_test != y_pred).sum()))\n",
    "print(\"Accuracy is \",  (y_test == y_pred).sum() / X_test.shape[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.980 accuracy.  That is good also.  Let's make the submission set. \n",
    "\n",
    "col1 = df_test['tweet_id'].tolist()\n",
    "y_pred = clf.fit(X_train, y_train).predict(test_images_to_array)\n",
    "labels = ['tweet_id', 'airline_sentiment']\n",
    "d = {'airline_sentiment': y_pred, 'tweet_id': col1}\n",
    "df = pd.DataFrame(data=d)\n",
    "df = df.reindex(columns=labels)\n",
    "df['airline_sentiment'] = df['airline_sentiment'].apply({-1 : 'negative', 0:'neutral', 1:'positive' }.get)\n",
    "\n",
    "df.to_csv('outputKNN.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 668 points : 13\n",
      "Accuracy is  0.9805389221556886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hongj\\Anaconda2\\envs\\Tensorflow\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Will SVM perform better?  Let's see. \n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "clf = svm.SVC()\n",
    "\n",
    "y_pred = clf.fit(X_train, y_train).predict(X_test)\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (X_test.shape[0],(y_test != y_pred).sum()))\n",
    "print(\"Accuracy is \",  (y_test == y_pred).sum() / X_test.shape[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hongj\\Anaconda2\\envs\\Tensorflow\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# 0.980.  Ok but not better than Naive Baysian.  Let's make the submission set anyway.  \n",
    "\n",
    "col1 = df_test['tweet_id'].tolist()\n",
    "y_pred = clf.fit(X_train, y_train).predict(test_images_to_array)\n",
    "labels = ['tweet_id', 'airline_sentiment']\n",
    "d = {'airline_sentiment': y_pred, 'tweet_id': col1}\n",
    "df = pd.DataFrame(data=d)\n",
    "df = df.reindex(columns=labels)\n",
    "df['airline_sentiment'] = df['airline_sentiment'].apply({-1 : 'negative', 0:'neutral', 1:'positive' }.get)\n",
    "\n",
    "df.to_csv('outputSVM.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 668 points : 17\n",
      "Accuracy is  0.9745508982035929\n"
     ]
    }
   ],
   "source": [
    "# How about decision tree?\n",
    "\n",
    "from sklearn import tree\n",
    "\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "\n",
    "y_pred = clf.fit(X_train, y_train).predict(X_test)\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (X_test.shape[0],(y_test != y_pred).sum()))\n",
    "print(\"Accuracy is \",  (y_test == y_pred).sum() / X_test.shape[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.974.  It is decreased.  \n",
    "\n",
    "col1 = df_test['tweet_id'].tolist()\n",
    "y_pred = clf.fit(X_train, y_train).predict(test_images_to_array)\n",
    "labels = ['tweet_id', 'airline_sentiment']\n",
    "d = {'airline_sentiment': y_pred, 'tweet_id': col1}\n",
    "df = pd.DataFrame(data=d)\n",
    "df = df.reindex(columns=labels)\n",
    "df['airline_sentiment'] = df['airline_sentiment'].apply({-1 : 'negative', 0:'neutral', 1:'positive' }.get)\n",
    "\n",
    "df.to_csv('outputDT.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 668 points : 11\n",
      "Accuracy is  0.9835329341317365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hongj\\Anaconda2\\envs\\Tensorflow\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Maybe random forrest?\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "y_pred = clf.fit(X_train, y_train).predict(X_test)\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (X_test.shape[0],(y_test != y_pred).sum()))\n",
    "print(\"Accuracy is \",  (y_test == y_pred).sum() / X_test.shape[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok it doesn't get better. \n",
    "\n",
    "col1 = df_test['tweet_id'].tolist()\n",
    "y_pred = clf.fit(X_train, y_train).predict(test_images_to_array)\n",
    "labels = ['tweet_id', 'airline_sentiment']\n",
    "d = {'airline_sentiment': y_pred, 'tweet_id': col1}\n",
    "df = pd.DataFrame(data=d)\n",
    "df = df.reindex(columns=labels)\n",
    "df['airline_sentiment'] = df['airline_sentiment'].apply({-1 : 'negative', 0:'neutral', 1:'positive' }.get)\n",
    "\n",
    "df.to_csv('outputRF.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where I stopped.  I believe I submitted the naive baysian one.  I thought my algorithm was pretty good, but when I saw the score, it was about 0.60.  I was very disappointed, and I thought my model was so overfitting.  I thought 0.60 was the accuracy.  \n",
    "\n",
    "I still do not know how my model has performed in the test set.  If you can tell me what was the accuracy, please let me know at hongjae79@gmail.com.  I am dying to curious how my model worked.  \n",
    "\n",
    "This model still has a room to improve.  I haven't run the neural network on this, which is the core of deep learning.  Also one can analyze further on rescaling probablity part to improve the result.  \n",
    "\n",
    "Also, I now read again and I see the code itself could have been written in simpler and shorter manner.  But I send the original anyway.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
